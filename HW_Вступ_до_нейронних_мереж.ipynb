{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GalynaDe/ML/blob/main/HW_%D0%92%D1%81%D1%82%D1%83%D0%BF_%D0%B4%D0%BE_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B8%D1%85_%D0%BC%D0%B5%D1%80%D0%B5%D0%B6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ],
      "metadata": {
        "id": "lbLHTNfSclli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ],
      "metadata": {
        "id": "GtOYB-RHfc_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3BNXSR-VdYKQ"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "QLKZ77x4v_-v"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Перетворення вхідних даних і цілей у тензори\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "metadata": {
        "id": "KjoeaDrk6fO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b47bce84-56ba-4786-f032-e25cf1e1e631"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n",
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
      ],
      "metadata": {
        "id": "iKzbJKfOgGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)"
      ],
      "metadata": {
        "id": "aXhKw6Tdj1-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b63b6af2-84cc-45b0-e7cb-f3b029f7bff3"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79ee668ddc30>"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.randn(1, 3, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "id": "eApcB7eb6h9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d6e1b5-40bc-456c-b189-c5b07d529ed0"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True)\n",
            "tensor([0.6213], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
        "  - обчсилення $e^x$: `torch.exp(x)`\n",
        "  - обчсилення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ],
      "metadata": {
        "id": "nYGxNGTaf5s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(x, w, b):\n",
        "    return 1/(1 + torch.exp(-(x @ w.t() + b)))"
      ],
      "metadata": {
        "id": "pSz2j4Fh6jBv"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs, w, b)\n",
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLTUa1O1MQVm",
        "outputId": "966244d8-e744-4397-a15e-ac307f7438bc"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
      ],
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(predicted_probs, true_labels):\n",
        "    loss = -(true_labels * torch.log(predicted_probs) + (1 - true_labels) * torch.log(1 - predicted_probs))\n",
        "    return torch.mean(loss)"
      ],
      "metadata": {
        "id": "1bWlovvx6kZS"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = binary_cross_entropy(preds, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oD07_3eMZVC",
        "outputId": "f0327f07-b020-4b10-9806-1df8af9233b2"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(nan, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ],
      "metadata": {
        "id": "ZFKpQxdHi1__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "w.grad, b.grad"
      ],
      "metadata": {
        "id": "YAbXUNSJ6mCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "239a784b-42b6-4414-b5a0-7da3b65bd019"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[nan, nan, nan]]), tensor([nan]))"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Градієнти зникли із-за насичення сигмоїдної функції активації. При великих позитивних значенях  z  сигмоїда наближається до 1, а для великих негативних до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля, що робить оновлення ваг неможливим."
      ],
      "metadata": {
        "id": "6JERSoMYM17t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "А я пишу пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ],
      "metadata": {
        "id": "nDN1t1RujQsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ],
      "metadata": {
        "id": "rOPSQyttpVjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
        "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
        "\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000"
      ],
      "metadata": {
        "id": "-EBOJ3tsnRaD"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs, w, b)\n",
        "loss = binary_cross_entropy(preds, targets)\n",
        "loss.backward()\n",
        "print(preds, loss, w.grad, b.grad)"
      ],
      "metadata": {
        "id": "-JwXiSpX6orh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "636e83c1-9b8e-4de0-a5ab-b836b47615f7"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5174],\n",
            "        [0.5220],\n",
            "        [0.5244],\n",
            "        [0.5204],\n",
            "        [0.5190]], grad_fn=<MulBackward0>) tensor(0.6829, grad_fn=<MeanBackward0>) tensor([[ -5.4417, -18.9853, -10.0682]]) tensor([-0.0794])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
        "\n",
        "  1. Генерація прогнозів\n",
        "  2. Обчислення втрат\n",
        "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
        "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
        "  5. Скидання градієнтів на нуль\n",
        "\n",
        "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
      ],
      "metadata": {
        "id": "RCdi44IT334o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-4\n",
        "\n",
        "for i in range(1000):\n",
        "    preds = model(inputs, w, b)\n",
        "    loss = binary_cross_entropy(preds, targets)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= w.grad * learning_rate\n",
        "        b -= b.grad * learning_rate\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()"
      ],
      "metadata": {
        "id": "mObHPyE06qsO"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs, w, b)\n",
        "loss = binary_cross_entropy(preds, targets)\n",
        "preds, targets, loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXdUN3JWOrpW",
        "outputId": "806a9eb8-e79a-4783-b08f-3f4208e9740d"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.4481],\n",
              "         [0.6883],\n",
              "         [0.9843],\n",
              "         [0.0038],\n",
              "         [0.9855]], grad_fn=<MulBackward0>),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.]]),\n",
              " tensor(0.2004, grad_fn=<MeanBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Передбачення достатньо непогані"
      ],
      "metadata": {
        "id": "HTzPlXxeO2UI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ],
      "metadata": {
        "id": "fuRhlyF9qAia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ],
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ],
      "metadata": {
        "id": "7X2dV30KtAPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "chrvMfBs6vjo"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "\n",
        "# Визначаємо dataset\n",
        "train_ds = TensorDataset(inputs, targets)\n",
        "train_ds[0:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pt5xamVJPIxy",
        "outputId": "8b9fcad1-5c09-4b23-8889-c8396f100fc4"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ],
      "metadata": {
        "id": "4nMFaa8suOd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Визначаємо data loader\n",
        "batch_size = 5\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "next(iter(train_dl))"
      ],
      "metadata": {
        "id": "ZCsRo5Mx6wEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4da66e61-f6e2-4941-dbf6-5ad77726b87f"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [102.,  43.,  37.],\n",
              "         [ 69.,  96.,  70.],\n",
              "         [102.,  43.,  37.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.],\n",
              "         [0.]])]"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ],
      "metadata": {
        "id": "ymcQOo_hum6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "EyAwhTBW6xxz"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Визначаємо модель\n",
        "class LogReg(nn.Module):\n",
        "    # Ініціалізуємо шари\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(3, 1)  # Лінійний шар\n",
        "        self.act = nn.Sigmoid()        # Функція активації\n",
        "\n",
        "    # Виконуємо обчислення\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "# Створюємо екземпляр моделі\n",
        "model = LogReg()"
      ],
      "metadata": {
        "id": "2JwXuPE6PXKU"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ],
      "metadata": {
        "id": "RflV7xeVyoJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.SGD(model.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "id": "3QCATPU_6yfa"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "loss_fn = F.binary_cross_entropy\n",
        "loss = loss_fn(model(inputs), targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyL6NjA4PiPX",
        "outputId": "52eba9b3-a2a5-47e2-89bc-387740ceb09d"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(7.6312, grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
      ],
      "metadata": {
        "id": "ch-WrYnKzMzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функція fit з відстеженням втрат\n",
        "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # Ініціалізуємо акумулятор для втрат\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_dl:\n",
        "            # Генеруємо передбачення\n",
        "            pred = model(xb)\n",
        "\n",
        "            # Обчислюємо втрати\n",
        "            loss = loss_fn(pred, yb)\n",
        "\n",
        "            # Виконуємо градієнтний спуск\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Накопичуємо втрати\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Обчислюємо середні втрати для епохи\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        # Виводимо підсумок епохи\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "    return losses"
      ],
      "metadata": {
        "id": "cEHQH9qE626k"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = fit_return_loss(1000, model, loss_fn, opt, train_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA54oU9MP5tU",
        "outputId": "cbc5fe3b-a7d0-4dc5-e91c-62800e581f53"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 1.3171\n",
            "Epoch [20/1000], Loss: 1.2595\n",
            "Epoch [30/1000], Loss: 1.2084\n",
            "Epoch [40/1000], Loss: 1.1388\n",
            "Epoch [50/1000], Loss: 1.0851\n",
            "Epoch [60/1000], Loss: 1.0415\n",
            "Epoch [70/1000], Loss: 0.9843\n",
            "Epoch [80/1000], Loss: 0.9393\n",
            "Epoch [90/1000], Loss: 0.8865\n",
            "Epoch [100/1000], Loss: 0.8436\n",
            "Epoch [110/1000], Loss: 0.8030\n",
            "Epoch [120/1000], Loss: 0.7632\n",
            "Epoch [130/1000], Loss: 0.7308\n",
            "Epoch [140/1000], Loss: 0.7033\n",
            "Epoch [150/1000], Loss: 0.6679\n",
            "Epoch [160/1000], Loss: 0.6437\n",
            "Epoch [170/1000], Loss: 0.6134\n",
            "Epoch [180/1000], Loss: 0.5918\n",
            "Epoch [190/1000], Loss: 0.5703\n",
            "Epoch [200/1000], Loss: 0.5492\n",
            "Epoch [210/1000], Loss: 0.5325\n",
            "Epoch [220/1000], Loss: 0.5149\n",
            "Epoch [230/1000], Loss: 0.5012\n",
            "Epoch [240/1000], Loss: 0.4870\n",
            "Epoch [250/1000], Loss: 0.4743\n",
            "Epoch [260/1000], Loss: 0.4654\n",
            "Epoch [270/1000], Loss: 0.4552\n",
            "Epoch [280/1000], Loss: 0.4421\n",
            "Epoch [290/1000], Loss: 0.4315\n",
            "Epoch [300/1000], Loss: 0.4263\n",
            "Epoch [310/1000], Loss: 0.4149\n",
            "Epoch [320/1000], Loss: 0.4085\n",
            "Epoch [330/1000], Loss: 0.4013\n",
            "Epoch [340/1000], Loss: 0.3940\n",
            "Epoch [350/1000], Loss: 0.3883\n",
            "Epoch [360/1000], Loss: 0.3829\n",
            "Epoch [370/1000], Loss: 0.3770\n",
            "Epoch [380/1000], Loss: 0.3719\n",
            "Epoch [390/1000], Loss: 0.3672\n",
            "Epoch [400/1000], Loss: 0.3627\n",
            "Epoch [410/1000], Loss: 0.3587\n",
            "Epoch [420/1000], Loss: 0.3549\n",
            "Epoch [430/1000], Loss: 0.3522\n",
            "Epoch [440/1000], Loss: 0.3474\n",
            "Epoch [450/1000], Loss: 0.3445\n",
            "Epoch [460/1000], Loss: 0.3411\n",
            "Epoch [470/1000], Loss: 0.3395\n",
            "Epoch [480/1000], Loss: 0.3350\n",
            "Epoch [490/1000], Loss: 0.3319\n",
            "Epoch [500/1000], Loss: 0.3297\n",
            "Epoch [510/1000], Loss: 0.3263\n",
            "Epoch [520/1000], Loss: 0.3239\n",
            "Epoch [530/1000], Loss: 0.3217\n",
            "Epoch [540/1000], Loss: 0.3199\n",
            "Epoch [550/1000], Loss: 0.3177\n",
            "Epoch [560/1000], Loss: 0.3154\n",
            "Epoch [570/1000], Loss: 0.3129\n",
            "Epoch [580/1000], Loss: 0.3109\n",
            "Epoch [590/1000], Loss: 0.3101\n",
            "Epoch [600/1000], Loss: 0.3079\n",
            "Epoch [610/1000], Loss: 0.3059\n",
            "Epoch [620/1000], Loss: 0.3047\n",
            "Epoch [630/1000], Loss: 0.3043\n",
            "Epoch [640/1000], Loss: 0.3011\n",
            "Epoch [650/1000], Loss: 0.2995\n",
            "Epoch [660/1000], Loss: 0.2983\n",
            "Epoch [670/1000], Loss: 0.2961\n",
            "Epoch [680/1000], Loss: 0.2949\n",
            "Epoch [690/1000], Loss: 0.2933\n",
            "Epoch [700/1000], Loss: 0.2921\n",
            "Epoch [710/1000], Loss: 0.2909\n",
            "Epoch [720/1000], Loss: 0.2908\n",
            "Epoch [730/1000], Loss: 0.2900\n",
            "Epoch [740/1000], Loss: 0.2878\n",
            "Epoch [750/1000], Loss: 0.2859\n",
            "Epoch [760/1000], Loss: 0.2858\n",
            "Epoch [770/1000], Loss: 0.2842\n",
            "Epoch [780/1000], Loss: 0.2840\n",
            "Epoch [790/1000], Loss: 0.2822\n",
            "Epoch [800/1000], Loss: 0.2810\n",
            "Epoch [810/1000], Loss: 0.2799\n",
            "Epoch [820/1000], Loss: 0.2788\n",
            "Epoch [830/1000], Loss: 0.2777\n",
            "Epoch [840/1000], Loss: 0.2771\n",
            "Epoch [850/1000], Loss: 0.2774\n",
            "Epoch [860/1000], Loss: 0.2752\n",
            "Epoch [870/1000], Loss: 0.2743\n",
            "Epoch [880/1000], Loss: 0.2733\n",
            "Epoch [890/1000], Loss: 0.2727\n",
            "Epoch [900/1000], Loss: 0.2731\n",
            "Epoch [910/1000], Loss: 0.2710\n",
            "Epoch [920/1000], Loss: 0.2711\n",
            "Epoch [930/1000], Loss: 0.2695\n",
            "Epoch [940/1000], Loss: 0.2686\n",
            "Epoch [950/1000], Loss: 0.2687\n",
            "Epoch [960/1000], Loss: 0.2683\n",
            "Epoch [970/1000], Loss: 0.2664\n",
            "Epoch [980/1000], Loss: 0.2668\n",
            "Epoch [990/1000], Loss: 0.2652\n",
            "Epoch [1000/1000], Loss: 0.2643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "3bDYXTaKP_Po",
        "outputId": "5ede667a-3ab3-4fb3-d9d0-648331e32a65"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQ7ZJREFUeJzt3Xl0VPXBxvFnJstk38kGgbDvhMhmBBUlikixbi1VXkXcqqJF6SZFodUqtn2t2oparUutCopVahWhgAsu7BA22bcEshFCMlknycx9/4iO5gViCJPcmcn3c86cM3Pn3skzlwN5uMvvZzEMwxAAAICfsJodAAAAwJMoNwAAwK9QbgAAgF+h3AAAAL9CuQEAAH6FcgMAAPwK5QYAAPiVQLMDtDeXy6X8/HxFRkbKYrGYHQcAALSAYRiqqKhQamqqrNbmj810uHKTn5+vtLQ0s2MAAIBWyMvLU5cuXZpdp8OVm8jISEmNOycqKsrkNAAAoCXsdrvS0tLcv8eb0+HKzTenoqKioig3AAD4mJZcUsIFxQAAwK9QbgAAgF+h3AAAAL9CuQEAAH7F1HKzatUqTZo0SampqbJYLFq8eHGLt/3iiy8UGBiooUOHtlk+AADge0wtN1VVVcrIyND8+fPPaLuysjLdeOONGjduXBslAwAAvsrUW8EnTJigCRMmnPF2d9xxh66//noFBASc0dEeAADg/3zumpuXX35ZBw4c0Ny5c1u0vsPhkN1ub/IAAAD+y6fKzd69e3X//ffrtddeU2Bgyw46zZs3T9HR0e4HUy8AAODffKbcOJ1OXX/99frd736nPn36tHi7WbNmqby83P3Iy8trw5QAAMBsPjP9QkVFhTZs2KDNmzfr7rvvltQ4w7dhGAoMDNR///tfXXzxxSdtZ7PZZLPZ2jsuAAAwic+Um6ioKG3btq3JsmeeeUYfffSR3n77bXXv3t2kZAAAwJuYWm4qKyu1b98+9+uDBw8qJydHcXFx6tq1q2bNmqWjR4/q1VdfldVq1aBBg5psn5iYqJCQkJOWm8HlMnS8qk6VjgZ1Twg3Ow4AAB2WqdfcbNiwQZmZmcrMzJQkzZw5U5mZmZozZ44kqaCgQLm5uWZGbLHP9pVoxCMrdOdrG82OAgBAh2YxDMMwO0R7stvtio6OVnl5uaKiojz2uXuLKnTJE6sUGRKobb8d77HPBQAAZ/b722fulvJ2KTGhkqSK2gZV1NabnAYAgI6LcuMhEbZARYU0XsJUUF5rchoAADouyo0HpX599ObV1YfMDQIAQAdGufGgi/slSpJW7iw2OQkAAB0X5caD7r64l6yWxtNSBeU1ZscBAKBDotx4UFhwoPolN17BvelwmblhAADooCg3HnZOtxhJ0ubcE+YGAQCgg6LceNg5XWMlSX///KC+3FdichoAADoeyo2HfVNuJOnBf283MQkAAB0T5cbDusWHuZ9HhwaZmAQAgI6JcuNhFotFj109WJIUFMDuBQCgvfHbtw10iW08elNWzTQMAAC0N8pNG4gJazwddaK6zuQkAAB0PJSbNhAbHiyp8chNB5t0HQAA01Fu2kB8eLCCA6yqc7q0p6jS7DgAAHQolJs2EBIUoPN7J0iSJv31czkanCYnAgCg46DctJGsnvGSpDqnS6+tyTU5DQAAHQflpo0M6RLjfl5QxiSaAAC0F8pNGxmR/u1IxS6uKQYAoN1QbtqIxWLR7Mv7S5KOVzlMTgMAQMdBuWlD8RGNt4SXVjHeDQAA7YVy04biI2ySpCJ7rclJAADoOCg3bahfcqQkaV9xpey1TMUAAEB7oNy0oaSoEHWLD5PLkD7ZfczsOAAAdAiUmzZ25dDOkqRFG/JMTgIAQMdAuWljlwxIkiTl5JXJxT3hAAC0OcpNG+ubHClboFUVtQ16Z/NRs+MAAOD3KDdtLCjAqomDUyRJy3YUmpwGAAD/R7lpBxOHNJabgnKmYQAAoK1RbtpBakyoJCm/jPFuAABoa5SbdpAa3VhuSqvqVF3XYHIaAAD8G+WmHUSFBiolOkSStHr/cZPTAADg3yg37cBisWj8wGRJ0gfbCkxOAwCAf6PctJNvLipe/lWRHA1Ok9MAAOC/KDftZFjXWCVHhaiitkGf7SkxOw4AAH6LctNOrFaLLu6fKElaf7jU5DQAAPgvyk076p8SJUnaXVhhchIAAPwX5aYd9UuOlES5AQCgLVFu2lHfr8tNQXmtDpVUmZwGAAD/RLlpR1EhQe7nk57+3MQkAAD4L8pNOxvcOVqSVFHbIMMwTE4DAID/ody0s9dvG+V+fryqzsQkAAD4J8pNO4sKCVJaXONcU3u4sBgAAI+j3JggMy1WkrT2IOPdAADgaZQbE5zbI16StOYAk2gCAOBplBsTnNsjTpK0ObdMlY4Gk9MAAOBfKDcm6J4Qrq5xYapzuvTy5wfNjgMAgF+h3JjAYrHohnO7SZJW7Co2OQ0AAP6FcmOSHp3CJUlb8sr07uYjJqcBAMB/mFpuVq1apUmTJik1NVUWi0WLFy9udv133nlHl1xyiTp16qSoqChlZWVp2bJl7RPWw7rEhrmf3/fmFhOTAADgX0wtN1VVVcrIyND8+fNbtP6qVat0ySWXaMmSJdq4caMuuugiTZo0SZs3b27jpJ7XJTa0yWtGKwYAwDMshpf8VrVYLHr33Xd15ZVXntF2AwcO1OTJkzVnzpwWrW+32xUdHa3y8nJFRUW1IqnnvLclXz9b0FjM1v1mnBKjQkzNAwCAtzqT398+fc2Ny+VSRUWF4uLiTruOw+GQ3W5v8vAWV2SkqnNM4xGcvBPVJqcBAMA/+HS5+d///V9VVlbqxz/+8WnXmTdvnqKjo92PtLS0dkz4/brGNV57k1tKuQEAwBN8tty88cYb+t3vfqe33npLiYmJp11v1qxZKi8vdz/y8vLaMeX3+2aeqbzSGpOTAADgHwLNDtAaCxcu1K233qpFixYpOzu72XVtNptsNls7JTtzHLkBAMCzfO7IzYIFCzRt2jQtWLBAEydONDvOWUv7utzkUW4AAPAIU4/cVFZWat++fe7XBw8eVE5OjuLi4tS1a1fNmjVLR48e1auvviqp8VTU1KlT9dRTT2nUqFEqLCyUJIWGhio6OtqU73C2usU3Dua3/1ilDMOQxWIxOREAAL7N1CM3GzZsUGZmpjIzMyVJM2fOVGZmpvu27oKCAuXm5rrXf/7559XQ0KDp06crJSXF/ZgxY4Yp+T2hX3KkAq0WlVTWqaC81uw4AAD4PK8Z56a9eNM4N9+Y+JfPtCPfrkeuGqQpo7qZHQcAAK/TYca58RdXZXaWJL2z6ajJSQAA8H2UGy9wcb/GW9m3Hy1Xg9NlchoAAHwb5cYLpMeHK9IWKEeDS7sKK8yOAwCAT6PceAGr1aJRPeIlScu/KjI5DQAAvo1y4yUuG5QsSVq2o9DkJAAA+DbKjZe4pH+SAqwW7SqsUEE5UzEAANBalBsvER0WpB4JjQP67ea6GwAAWo1y40V6J0VIkvYWVZqcBAAA30W58SIDUxunkHh/a77JSQAA8F2UGy8yeUSarBZpy5FyFTIVAwAArUK58SIJETb1SYqUJOXklZkbBgAAH0W58TKZXWMkSVuOlJmaAwAAX0W58TIZXWIkSc9+sl8HS6rMDQMAgA+i3HiZjLQY9/Nrn/3SvCAAAPgoyo2X6fv1NTeSdLyqzsQkAAD4JsqNl7FaLbp2WBf3a2YJBwDgzFBuvNBjVw92Py+t5ugNAABngnLjhQIDrEqIsEmSiu0Ok9MAAOBbKDdeKjUmRJK0YmeRyUkAAPAtlBsv9ZMRXSVJf/1on46cqDY5DQAAvoNy46V+MiJNnWNC5XQZ2pFvNzsOAAA+g3LjpaxWi4anx0qSXlh1wOQ0AAD4DsqNF/tmnqkNh0+ojLumAABoEcqNF7t+ZFf38yMnakxMAgCA76DceLHY8GBldImWJOWXUW4AAGgJyo2XS4kOlSQVlNeanAQAAN9AufFyXWIby82O/HKTkwAA4BsoN15uXP8kSdKH2wpVU+c0OQ0AAN6PcuPlRnWPU5fYUFU4GhitGACAFqDceDmr1aJx/RIlSduPcmoKAIDvQ7nxAb0SIyRJ+49VmpwEAADvR7nxAT07NZabdQdLVV5db3IaAAC8G+XGB5zTLVadY0Jlr23Qh9sLzI4DAIBXo9z4gJCgAF0xNFWStOVImblhAADwcpQbH5HRJUaStGBdHqMVAwDQDMqNj8hIi3Y/v/fNHPOCAADg5Sg3PiI5KsT9fN3BUhOTAADg3Sg3PsJisejXl/WTJMWEBZmcBgAA70W58SGTR6RJksqq6+VoYCoGAABOhXLjQ2LDgmQLbPwjW7z5qAzDMDkRAADeh3LjQywWi87vnSBJ+vW/tum1tbkmJwIAwPtQbnzMD4d2dj9/cPF2E5MAAOCdKDc+5sK+nZq85tQUAABNUW58TFRIkD742Rj36zLmmgIAoAnKjQ8amBqtuPBgSVKhvdbkNAAAeBfKjY/6ZlC/wnLKDQAA30W58VEp0Y3l5siJapOTAADgXSg3Pqp/SpQkaeuRcpOTAADgXSg3PmpoWowkacPhE+YGAQDAy1BufNTIHnEKCrDoYEmVNudScAAA+Iap5WbVqlWaNGmSUlNTZbFYtHjx4u/d5pNPPtE555wjm82mXr166ZVXXmnznN4oKiRIw7vFSZKueuZLLiwGAOBrppabqqoqZWRkaP78+S1a/+DBg5o4caIuuugi5eTk6N5779Wtt96qZcuWtXFS73T5kBT38y/2lZiYBAAA7xFo5g+fMGGCJkyY0OL1n3vuOXXv3l2PP/64JKl///76/PPP9cQTT2j8+PGn3MbhcMjhcLhf2+32swvtRSYPT3NPwZCTV6ZrhnUxOREAAObzqWtuVq9erezs7CbLxo8fr9WrV592m3nz5ik6Otr9SEtLa+uY7SY40Kr5158jSdqcx3U3AABIPlZuCgsLlZSU1GRZUlKS7Ha7ampqTrnNrFmzVF5e7n7k5eW1R9R2M7RrjCRpZ0GFauqc5oYBAMALmHpaqj3YbDbZbDazY7SZ1OgQJUbaVFzh0Pb8co1IjzM7EgAApvKpIzfJyckqKipqsqyoqEhRUVEKDQ01KZW5LBaLMr8+erPhEKemAADwqXKTlZWllStXNlm2fPlyZWVlmZTIO4zplSBJem3NYTldhslpAAAwl6nlprKyUjk5OcrJyZHUeKt3Tk6OcnNzJTVeL3PjjTe617/jjjt04MAB/epXv9KuXbv0zDPP6K233tJ9991nRnyv8aPhaYoJC9LRshqt2nvM7DgAAJjK1HKzYcMGZWZmKjMzU5I0c+ZMZWZmas6cOZKkgoICd9GRpO7du+uDDz7Q8uXLlZGRoccff1x///vfT3sbeEcREhSgCYMax7xZs/+4yWkAADCXqRcUjx07VoZx+tMopxp9eOzYsdq8eXMbpvJNgztHa4GknYUVZkcBAMBUPnXNDU6vX0qkJGlngf8MUggAQGtQbvxE36RIWSzSsQqHSiod378BAAB+inLjJ8JtgeoaFyZJuv3VDSanAQDAPJQbP5IeHy5Jyi099WjNAAB0BJQbP/L4jzMkSSWVDlXXNZicBgAAc1Bu/EhChE3RoUGSpMPHq01OAwCAOSg3fqZvUuNdU+sPlZqcBAAAc1Bu/MylAxtnTX9745FmxxACAMBfUW78zJWZnWULtGrrkXKtO8jRGwBAx0O58TMJETZdM6yLJOmWf2xQeXW9yYkAAGhflBs/dNv5PSRJlY4GvfzlQZPTAADQvig3fqh7QriGdYuVJO0trjQ5DQAA7Yty46d+ekHj0Zu8Um4JBwB0LJQbP9U1vnEqhkMlVdw1BQDoUCg3fqp7QriCA62y1zboYEmV2XEAAGg3lBs/ZQsM0NC0GEnSR7uKzQ0DAEA7otz4sR8OTZUkvfLlIXODAADQjig3fuzKoZ1ltUhHTtSooJyZwgEAHQPlxo+F2wI1IDVKkvTZnhKT0wAA0D4oN35uwqAUSdLC9blyubhrCgDg/yg3fu6ac7rIYpE25Zapx2+WqKbOaXYkAADaFOXGzyVHh6h7fLj79d7iChPTAADQ9ig3HcDvrxrkfl5sd5iYBACAtke56QDO65mgMb0SJEnrDpUyYjEAwK9RbjqIbl9Px/D8qgN6f2uByWkAAGg7lJsOomtcmPv5Yx/uMjEJAABti3LTQfwgI9X9/JujOAAA+CPKTQfROSZUt4zpLkmqqed2cACA/6LcdCAThzQO6FdUXmtyEgAA2g7lpgPpHh8uq0XKL69VXmm12XEAAGgTlJsOJDY8WCPS4yRJH+8uNjkNAABtg3LTwYz+Zrybg6UmJwEAoG1QbjqYc3vES5KWf1Wk19ce1tLtjHkDAPAvlJsOZkR6rEb3ipejwaXZ727XHa9tUpGdC4wBAP6DctPBWCwW/eby/k2WlVQy3xQAwH+0qtzk5eXpyJEj7tfr1q3Tvffeq+eff95jwdB2BqRENXldXlNvUhIAADyvVeXm+uuv18cffyxJKiws1CWXXKJ169Zp9uzZeuihhzwaEJ5nsViavC6rptwAAPxHq8rN9u3bNXLkSEnSW2+9pUGDBunLL7/U66+/rldeecWT+dBGfjy8i/t5aVWdiUkAAPCsVpWb+vp62Ww2SdKKFSt0xRVXSJL69eunggLuvvEFcyYNdD8/QbkBAPiRVpWbgQMH6rnnntNnn32m5cuX67LLLpMk5efnKz4+3qMB0TYibIGaMa63JGn5ziLVO10mJwIAwDNaVW7+8Ic/6G9/+5vGjh2r6667ThkZGZKk9957z326Ct5v0tczhW89Uq6F63JNTgMAgGcEtmajsWPHqqSkRHa7XbGxse7lt99+u8LCwjwWDm2rV2KELh+crCXbCrXlSLluMDsQAAAe0KojNzU1NXI4HO5ic/jwYT355JPavXu3EhMTPRoQbeuKjM6SpJ0FdpOTAADgGa0qNz/84Q/16quvSpLKyso0atQoPf7447ryyiv17LPPejQg2lZGWrQk6asCOzOFAwD8QqvKzaZNm3T++edLkt5++20lJSXp8OHDevXVV/WXv/zFowHRtlKiQzW6V7wMQ/pgG3e6AQB8X6vKTXV1tSIjIyVJ//3vf3X11VfLarXq3HPP1eHDhz0aEG3vsoHJkqS3Nx5RXQN3TQEAfFuryk2vXr20ePFi5eXladmyZbr00kslScXFxYqKivqereFtJg5JVXRokPYVV+rTPcfMjgMAwFlpVbmZM2eOfvGLXyg9PV0jR45UVlaWpMajOJmZmR4NiLYXFx6sywenSJJ++94OVToaTE4EAEDrtarcXHvttcrNzdWGDRu0bNky9/Jx48bpiSee8Fg4tJ+L+zXe5Xa0rEZ/WbnX5DQAALReq8qNJCUnJyszM1P5+fnuGcJHjhypfv36ndHnzJ8/X+np6QoJCdGoUaO0bt26Ztd/8skn1bdvX4WGhiotLU333XefamtrW/s18LXs/om6dUx3SVJOXpm5YQAAOAutKjcul0sPPfSQoqOj1a1bN3Xr1k0xMTF6+OGH5XK1/ILUN998UzNnztTcuXO1adMmZWRkaPz48SouLj7l+m+88Ybuv/9+zZ07Vzt37tSLL76oN998U7/5zW9a8zXwHRaLRVcMbRyx+MCxKpPTAADQeq0qN7Nnz9bTTz+txx57TJs3b9bmzZv16KOP6q9//asefPDBFn/On//8Z912222aNm2aBgwYoOeee05hYWF66aWXTrn+l19+qdGjR+v6669Xenq6Lr30Ul133XXfe7QHLdOjU4QkqaTSoetfWKO31ueZnAgAgDPXqnLzj3/8Q3//+9915513asiQIRoyZIjuuusuvfDCC3rllVda9Bl1dXXauHGjsrOzvw1jtSo7O1urV68+5TbnnXeeNm7c6C4zBw4c0JIlS3T55Zef9uc4HA7Z7fYmD5xahC1Q5/VsnPj0y/3H9at/bTU5EQAAZ65V5aa0tPSU19b069dPpaWlLfqMkpISOZ1OJSUlNVmelJSkwsLCU25z/fXX66GHHtKYMWMUFBSknj17auzYsc2elpo3b56io6Pdj7S0tBbl66j+59xuTV47GpwmJQEAoHVaVW4yMjL09NNPn7T86aef1pAhQ8461Ol88sknevTRR/XMM89o06ZNeuedd/TBBx/o4YcfPu02s2bNUnl5ufuRl8epluZcMqBp2Swqd5iUBACA1mnVrOB//OMfNXHiRK1YscI9xs3q1auVl5enJUuWtOgzEhISFBAQoKKioibLi4qKlJycfMptHnzwQd1www269dZbJUmDBw9WVVWVbr/9ds2ePVtW68ldzWazyWazncnX69CCAqx6cvJQ3ftmjiTpSFm1usYz0zsAwHe06sjNhRdeqD179uiqq65SWVmZysrKdPXVV2vHjh365z//2aLPCA4O1rBhw7Ry5Ur3MpfLpZUrV7oL0/9XXV19UoEJCAiQJBmG0ZqvglO4MrOzxvbtJEnaX1xpchoAAM5Mq47cSFJqaqoeeeSRJsu2bNmiF198Uc8//3yLPmPmzJmaOnWqhg8frpEjR+rJJ59UVVWVpk2bJkm68cYb1blzZ82bN0+SNGnSJP35z39WZmamRo0apX379unBBx/UpEmT3CUHnjGkc7Q+2X1MW46U6wazwwAAcAZaXW48YfLkyTp27JjmzJmjwsJCDR06VEuXLnVfZJybm9vkSM0DDzwgi8WiBx54QEePHlWnTp00adKkk0oWzt7QrjGSpLUHj8swDFksFnMDAQDQQhbDg+dztmzZonPOOUdOp/feYWO32xUdHa3y8nIm+WxGlaNBmQ8vV12DSytmXqBeiZFmRwIAdGBn8vu71dMvwL+F2wKV1aNxzJvX1uSanAYAgJY7o9NSV199dbPvl5WVnU0WeJkJg5L16Z5jem3NYd1+QQ+lxoSaHQkAgO91RkduvjsY3qke3bp104033thWWdHOfjw8TUlRNjW4DK0/1LLBGQEAMNsZHbl5+eWX2yoHvJDVatEPhqTqxc8PasG6XF2RkcqFxQAAr8c1N2jWDed2U0iQVWsOlOrh93eaHQcAgO9FuUGz0hPCNX1sL0nSq6sPqcrRYHIiAACaR7nB97pnXG/FhgWpwWVoc26Z2XEAAGgW5QYtclHfREmNg/oBAODNKDdokZHd4yRJ72w6qtp67x2kEQAAyg1aZFz/JIUFB+hoWY2WbCswOw4AAKdFuUGLdIq06YasbpKkD7ZSbgAA3otygxb7weBUWSzSyl3F2nqkzOw4AACcEuUGLTa4S7TGD0iWJF3x9Bf6cn+JyYkAADgZ5QZn5LpRXd3P/70538QkAACcGuUGZ+TCPp10y5jukqSDJVUmpwEA4GSUG5yxK4d2liStO1Sq8up6k9MAANAU5QZnrHdShDpF2iRJT3+81+Q0AAA0RbnBGQsJCtBDVwyU1Dion8tlmJwIAIBvUW7QKtkDkhRpC9Txqjot21FodhwAANwoN2iVoACrJg5JkST99j87mJIBAOA1KDdotTmTBig5KkRFdofufmOTKh0NZkcCAIByg9YLCw7UnWN7SpJW7CzWUyv2mJwIAADKDc7S5BFp7uf/2cKcUwAA81FucFZCggL02i2jJEmVjgbunAIAmI5yg7M2snucIm2BqnQ06L9fcecUAMBclBucteBAq6ac202SNP/j/TIMjt4AAMxDuYFHTBjUOFv4tqPlem8LE2oCAMxDuYFHDOkSrYGpUZKkD7dxagoAYB7KDTzCYrFozg8GSJKW7ijUp3uOmZwIANBRUW7gMSO7x+naYV0kSXP/vV2OBkYtBgC0P8oNPMZisWjupAFKiAjWoePV+mJfidmRAAAdEOUGHhUZEqRLBiRJarxzysm4NwCAdka5gcddO6yLLBZp4+ETWrQhj1vDAQDtinIDjxvWLU6/uLSvJOn+d7bpJ8+vMTkRAKAjodygTdwyprv7+dqDpapixnAAQDuh3KBNhAQF6B83j3S/3lNUYWIaAEBHQrlBm7mwTydd0KeTJOn6F9aqtp5bwwEAbY9ygzY1eXiaJKmm3qkPthaYnAYA0BFQbtCmLh+crB6dwiVJP1+0RaVVdSYnAgD4O8oN2pTFYtHjP8pwv/7Tsl0mpgEAdASUG7S5zK6xSo8Pk9Q49g0AAG2JcoN28cZt50qS9h+rUkmlw+Q0AAB/RrlBu0iJDlHvxAg5XYZueWW9XEzLAABoI5QbtAuLxaI/fX3tzZYj5frP1nyTEwEA/BXlBu1maFqMfjKi8dbwue/tUL3TZXIiAIA/otygXf1mYn+FBweorLpevWd/qK/y7WZHAgD4GcoN2lVUSJAe+uEg9+tHl+w0MQ0AwB9RbtDurhnWRVef01mS9Pm+EhWW15qcCADgTyg3MMW8qwcrKMAiSbrkiU9VXces4QAAzzC93MyfP1/p6ekKCQnRqFGjtG7dumbXLysr0/Tp05WSkiKbzaY+ffpoyZIl7ZQWnmILDNDMS/pKkipqG7ThEIP7AQA8w9Ry8+abb2rmzJmaO3euNm3apIyMDI0fP17FxcWnXL+urk6XXHKJDh06pLffflu7d+/WCy+8oM6dO7dzcnjCnWN76ppzukiSZr2zTXuKKkxOBADwBxbDMEwbTW3UqFEaMWKEnn76aUmSy+VSWlqa7rnnHt1///0nrf/cc8/pT3/6k3bt2qWgoKBW/Uy73a7o6GiVl5crKirqrPLj7K3ac0w3vtR4tC49Pkwf/2KsLBaLyakAAN7mTH5/m3bkpq6uThs3blR2dva3YaxWZWdna/Xq1afc5r333lNWVpamT5+upKQkDRo0SI8++qicTudpf47D4ZDdbm/ygPcY0ytBt47pLkk6dLxaB0qqTE4EAPB1ppWbkpISOZ1OJSUlNVmelJSkwsLCU25z4MABvf3223I6nVqyZIkefPBBPf744/r9739/2p8zb948RUdHux9paWke/R44O1arRQ/8YIBG94qXJD25Yq/JiQAAvs70C4rPhMvlUmJiop5//nkNGzZMkydP1uzZs/Xcc8+ddptZs2apvLzc/cjLy2vHxGipX1/WT5L0ny35+nJficlpAAC+zLRyk5CQoICAABUVFTVZXlRUpOTk5FNuk5KSoj59+iggIMC9rH///iosLFRdXd0pt7HZbIqKimrygPcZ0iVGI7vHSZJu+ccGnag69Z8nAADfx7RyExwcrGHDhmnlypXuZS6XSytXrlRWVtYptxk9erT27dsnl+vbOYn27NmjlJQUBQcHt3lmtK35158jq0WqqXfq4Q++MjsOAMBHmXpaaubMmXrhhRf0j3/8Qzt37tSdd96pqqoqTZs2TZJ04403atasWe7177zzTpWWlmrGjBnas2ePPvjgAz366KOaPn26WV8BHtQp0qbZEwdIkt7ZdFTDf79c5TX1JqcCAPiaQDN/+OTJk3Xs2DHNmTNHhYWFGjp0qJYuXeq+yDg3N1dW67f9Ky0tTcuWLdN9992nIUOGqHPnzpoxY4Z+/etfm/UV4GE3j07X6v0lWrGzWCWVdXrli0Oakd3b7FgAAB9i6jg3ZmCcG+9XUunQ8N+vkCT1SozQhzPOV1CAT137DgDwMJ8Y5wY4nYQIm9bPzlZkSKD2FVfqieV7zI4EAPAhlBt4pU6RNv3xmiGSpGc/3a81B46bnAgA4CsoN/BaEwanaPLwNBmGdO/CHJVVc3s4AOD7UW7g1X4+vo9sgVYV2mv1039uVAe7RAwA0AqUG3i1xMgQvXrzSAUHWLX2YKlW7WX0YgBA8yg38HqjesTrmmFdJEm3v7pBy3YU6iATbAIAToNyA58w6/J+6psUKUeDSz/950Zd9L+fqLii1uxYAAAvRLmBT4gKCdLfpw5vsmzNgVKT0gAAvBnlBj4jLS5Mb9w2yv3635uPyuXiAmMAQFOUG/iU83om6F93nqegAItW7irWH5buMjsSAMDLUG7gc4Z1i9Ufvh7g72+rDujcR1fqRBVj4AAAGlFu4JOuPqeLfjm+rySp0F6rRRvzTE4EAPAWlBv4rLvG9tRFfTtJkl747CAjGAMAJFFu4MMsFovmTzlHPTuF61iFQxOe+kyvrz2sugaX2dEAACai3MCnhQUH6rFrhig0KEAF5bWa/e52/XPNYbNjAQBMRLmBzxuRHqf3fzbG/fqdTUfkaHCamAgAYCbKDfxCz04R+vL+ixUcaNWOfLt+uWir2ZEAACah3MBvpMaE6u83DpfFIr23JV9/XLqLIzgA0AFRbuBXLujTSZOHp0mSnvlkv3703Gr9c81h1Tu5yBgAOgqLYRgdavx6u92u6OholZeXKyoqyuw4aAP1TpfeWJureR/uVG19Y6lJjw/Typ+PVYDVYnI6AEBrnMnvb47cwO8EBVg19bx0PTtlmHvZoePV+nfOURNTAQDaC+UGfuuifolacNu57tdvrM2Vk4k2AcDvUW7g17J6xuvVm0dKkjYcPqEnlu8xOREAoK1RbuD3LujTSU/9ZKgk6ZlP9unypz7TtJfXqbii1txgAIA2QblBh/DDoZ119Tmd5TKkrwrs+nj3Mf1l5V6zYwEA2gDlBh3G4z/K0MNXDnK//u+OItXWMw4OAPgbyg06DIvFohvO7aY3b2+8yLi4wqFrnv1SH+8q1tYjZeaGAwB4DOUGHc6oHvF6/oZhirAFake+XdNeWa8rnv5COwvsZkcDAHgA5QYd0qUDk/XWT7OaLJvw1Gfaf6zSpEQAAE+h3KDDGpAapQ9+NkYJEcHuZb95Z5s62KDdAOB3KDfo0AamRmvtb7L1gyEpkqS1B0t1+z83MuEmAPgwyg06vACrRU9ff47+eO0QSdLyr4r0wLvblVdarboGJtwEAF8TaHYAwFv8eHiaAq0WzXxrixZtPKJFG49Ikl66abgu7pdkcjoAQEtx5Ab4jqvP6aK/3TBMXePC3MtufmWDLntylfLLakxMBgBoKcoN8P+MH5isT34xVr+dNMC9bFdhhX7+1hYTUwEAWopyA5yC1WrRTaO768nJQ93LVh84rlv/sUFPrtijmjouOAYAb0W5AZpxZWZnLfx6RGNJWrGzSE+u2KuH3v/KxFQAgOZQboDvcW6PeO3+/WX62bje7mUL1uVq6kvr5HIxJg4AeBuL0cFGLLPb7YqOjlZ5ebmioqLMjgMfdO/CzVqck+9+PaRLtJ77n2FKjQk1MRUA+Lcz+f3NkRvgDP3uh4PULf7bu6m2HinXH5buMjERAOC7KDfAGYoODdKnv7xIf7thmHvZv3PyNfZPH2v1/uMmJgMASJyWMjsO/MAfl+7SM5/sd7/u0Slcd43tpWuHdTExFQD4F05LAe3ol+P7at7Vg92vDxyr0ux3t+lEVZ2JqQCg4+LIDeAhTpehpz/apydW7JEkRdoCVeFo0HUj0/ToVYNlsVhMTggAvosjN4AJAqwWzcjurX9PH62encJV4WiQJC1Yl6fesz/UUyv2mpwQADoGyg3gYRlpMVp67wX63RUD3csaXIaeWLFH72/N14ZDpdpZYDcxIQD4N05LAW2ott6pv360V/M/3t9keXhwgDY8cIlCgwNMSgYAvoXTUoCXCAkK0C/H99OGB7L1kxFpCgpovO6mqs6p/nOW6qNdRZKkDvZ/DABoUxy5AdpReXW97nsrRx/tKnYvCw8OaCw7KVFadEeWImyBJiYEAO/kc0du5s+fr/T0dIWEhGjUqFFat25di7ZbuHChLBaLrrzyyrYNCHhIdFiQXrpphBbc9u1knFVfzzC+s8Cucx9dqUMlVWbFAwC/YHq5efPNNzVz5kzNnTtXmzZtUkZGhsaPH6/i4uJmtzt06JB+8Ytf6Pzzz2+npIDnZPWM18F5l2vlzy/UrWO6u5dXOho09n8/0X1v5qi0qk619U4TUwKAbzL9tNSoUaM0YsQIPf3005Ikl8ultLQ03XPPPbr//vtPuY3T6dQFF1ygm2++WZ999pnKysq0ePHiFv08TkvBG+Uer9YD/96uVXuOnfTeXWN76t7sPgoONP3/IgBgGp85LVVXV6eNGzcqOzvbvcxqtSo7O1urV68+7XYPPfSQEhMTdcstt3zvz3A4HLLb7U0egLfpGh+mV28eqe2/G69po9MVEvTtX81nPtmvPg98qF8u2qJ6p8vElADgG0wtNyUlJXI6nUpKSmqyPCkpSYWFhafc5vPPP9eLL76oF154oUU/Y968eYqOjnY/0tLSzjo30FYibIGaO2mgNjxwie4a27PJe4s2HtGdr23SJ7uLNe3lde47rQAATfnUce6KigrdcMMNeuGFF5SQkNCibWbNmqXy8nL3Iy8vr41TAmcvwhaoX13WT7t/f1mT2cdX7CzSTS+v18e7j+nmVzbok93Fcro61A2PAPC9TL3nNCEhQQEBASoqavo/0KKiIiUnJ5+0/v79+3Xo0CFNmjTJvczlajxMHxgYqN27d6tnz6b/27XZbLLZbG2QHmh7tsAAjR+YrEOPTdTS7QV6auW+JqMb3/Tyeg1Ni9E153TW+IHJCrcFKpxbyQF0cF5xQfHIkSP117/+VVJjWenatavuvvvuky4orq2t1b59+5ose+CBB1RRUaGnnnpKffr0UXBwcLM/jwuK4evySqu15UiZ7v/XNlV+PX/Vdy247Vxl9Yw3IRkAtJ0z+f1t+n/xZs6cqalTp2r48OEaOXKknnzySVVVVWnatGmSpBtvvFGdO3fWvHnzFBISokGDBjXZPiYmRpJOWg74q7S4MKXFhekHQ1L13x2FemTJTh0+Xu1+/7oX1ujcHnHK7p+kG7PSucsKQIdjermZPHmyjh07pjlz5qiwsFBDhw7V0qVL3RcZ5+bmymrlH2fgVC4dmKxLBybreKVDqw8c1yMf7FRBea3WHCjVmgOl+v0HO3XTeemalJGiKodTA1KjlBDBaVoA/s3001LtjdNS8Hd5pdX6w9Jden9rwSnfT48P06I7zlOnSEoOAN9xJr+/KTeAn6qua9Dq/cf1zzWH9cnupoMDWixSdGiQ0mLDNGNcb2UPSDrNpwCAd6DcNINyg46ovKZeMqT739mqD7efPIbUpIxUXZ3ZWf1SIpUQYVNQAKeCAXgXyk0zKDfo6GrqnFq0MU//2nRUW/LKTrnO+IFJuiKjsyYOSWnfcABwGpSbZlBugG85Gpz6eFex3t9aoKXbC9Xw/wYEHNu3k0akx2lk9zjtK67U+IHJigtvfrgFAGgLlJtmUG6AU6utd2r1geN6csXe0x7RkaTJw9M0bUy6+iZFymKxtF9AAB0a5aYZlBugZbYfLdfag6X6eFexPt9XctL7fZMiNbZvJx0oqdLF/RL1o2FdFMi1OgDaCOWmGZQb4MwZhqGC8lrd+2aOCstrlXeiWqf6lyM9PkwDU6OVFBWi9IQwFdlrNfOSvgqwcoQHwNnxqRGKAXg/i8Wi1JhQvfXTLElSflmNPtpVrA2HSvXRrmLZaxungTh0vFqHvjNasiQdq3DokgHJmv3uNl11TmfNmtC/3fMD6Fg4cgPgrB0tq9GCtbnaVVihHfnlKiivPe26PRLCdc2wLjpW4dDNo7srMKCxOK0/VKqY0CD1Topsx+QAfAWnpZpBuQHa3rEKhzbnntDS7YXKOVKmA8eqTrtuoNWiW8/voec+3S9JWvnzC9WzU0R7RQXgIyg3zaDcAO3PMAyVVdfr1dWH9c81h1VS6fjebUamx+nmMd11QZ8EhQQGyGUYXLAMdGCUm2ZQbgDvcLCkSu/l5OvD7QU6UFKlugbX924zbXS6LuzTSW9vPKJLByZr0pAU/XPNYaVGhzKFBODnKDfNoNwA3qm23qn9xyr10c5i7Sqs0NqDx1VSWdfsNoFWi3vgwZvOS9evLuursGDukwD8EeWmGZQbwDcYhqHSqjodOl6tT3cXK7e0WvnltVp3sPS028SHB6trfJisFou2HS3Xj4d30Yj0OF0+OIX5sgAfR7lpBuUG8G1F9lrV1jv1+tpc7Syw67O9Jw8weCo9EsLVKzFClw1KVlCAVaFBARrdK0FWq2QLDGjj1ADOFuWmGZQbwD/lHq/W5rwTOnCsSku2FWhvcWWLtgsOsGrqed1UXedUZtdYXTusiyTJ6TLU4HJRfAAvQblpBuUG6Dhq6pzakd84jUSlo0HHKx36Yt9xHS2rOe02VosUH2HTsYrGO7p+ekEPpUSHaHCXaKXFhSmvtFo9EiL0+w92anSveJ3XM0HJ0SHt9ZWADoty0wzKDQBJqmtwaUd+ud7fWqDc0mot/6qoVZ8TYLXoyclDFR0apIGpUVq5s1hZPeO1/1ilLuzTiclFAQ+h3DSDcgPgdOy19co9Xq280mrtLqpQXmnjEZ7dRXYdPFalqjrnGX9mclSIbhqdrnH9EpUWF6aQIE5zAa1BuWkG5QZAaxiGofzyWh2vdOjL/ce17mCp6p0ufZVvl6PBpUpHQ4s+JzYsSOf1TJAsUkmFQ9n9k5QaE6ro0CCd1zNeVqtFOXllOlhSqSuHdpYkjv4Aotw0i3IDoC2cqKrT2oOl2na0TFUOpzblntDWI+WSJItFsgVaVVvf/ECFUSGBigsPPmny0XN7xGlSRqqGd4tT3+RIlVfXq97lUnx4sEqr6mS1WBQbHtxm3w3wBpSbZlBuAJihwenSF/uPq6K2Xqv3H5e9tkF7iyqUFBWiT/cca/HndE8I18GSpnN1WSzSPRf3VpfYUMWGBSsmLEi9EyMUE0bhgf+g3DSDcgPA29TWO1XlaNDWI+VyGYaq6pw6UVWnnQV2LVyfJ0mKCQtSWXX9GX1uQkSwYsKC3VNbTMpI0e7CSh2vcugP1wxR78QIOV2GHA0uhdsY2RnejXLTDMoNAF9kGIb2FFWq0F6r4ACrPt5drNp6pxz1Lu0prlCAxaIT1XXa38wM7M3p2Slc3RMi9NnexqNIDS5DsWFBurhfos75evyf001c6nIZMtR45xjQVig3zaDcAPBnJ6rqdLSsRnml1TpW6dD2o+VyNLi04dAJWa1y3wHWGunxYbIFBuhYpUOlVY3zfoUGBaim3qnESJuiQoMUHhygl6eNVGxYkI5VOhQVEsQdYvAIyk0zKDcAOjrDMHSwpErVdU7lllZrV4Fd0WHBcrpcOnCsSp/tLWl2oMMzEWi1qHtCuBKjbFp/8ITO7Rmvkemxqnca6tEpXPbaBo3rl6jUmFAdOVGtdzYd1dSsdEWHBUlqPCpksXDHGCg3zaLcAEDLGN+5/qek0qHymnodOVGjitoGbThUqrKaep2oqlNBea1q6s98DKDv6tEpXEdO1LivD/rphT308a5i7SlqnEbj2mFdNKZXgmyBVkWHBikyJEihwQFKjw9TYIBV9U6XymvqlRBhO+vvDe9EuWkG5QYA2k5BeY2cLkMulxQREqgPtxeooKyx/Pxz9WHFhAWp+OupLeK+vpX9bHWK/Ha6DElKiQ5RUIBVF/XtpJCgAPVNjlSVo0G2wADFRwSrU6RNaw+Uqmt8mMYPTD7t59Y1uBQcyGzy3oJy0wzKDQB4D8MwlFdao9zSapXV1MnpMlRQXqujJ2rkMgwVVzi0s8CuAKtFheW1cjQ0P1bQmeqTFKEeCRGSpNSYUJXX1Otfm46437+4X6IuHZCkBpehuPBg5ZfV6EfD0xQdGtTkOzgaXFxb1MYoN82g3ACAbzMMQzX1TlktFh06XqWCslq5DEN5pdU6XFqtAItFh45XKz0+TIeOV2tT7gmPHCH6roQIm2LCgmSvqXffot81PkydY0IVERKoAItFKdEhanAZGpEep06RNiVG2nS0rEYnquo0qHO0YsKCdKzCobjwYMYkagHKTTMoNwDQcdU1uHS8yqGaOmdjMSqvlcuQjtlrVVzhUGhwgCpqG1ReUy/DaJxXrLy6XpWOBrna8Ldlz07h6pccpYMlVbJapaTIEJVW1ynCFqhh3WIVGRKk5z7dr/DgANkCA/Szcb2VFheqAKtFxRUOHTxWpSszO6u23qnkqBBZ/fC2fMpNMyg3AIDWMAxDTpeh/LJaHSmrVr3TULWjQUX2WgUHBujE12UkJ69Mh49XqVOkTYYh7Sqs+Pp0W02bFqRvpMWFalBqtHLyyhQSFKDESJvqnS71ToxUZtcYBQda9fm+EnWKtCnAYlHn2FD9aFiadhbYFRcerM4xoaqsa1BYUIACrBavuVONctMMyg0AwAyGYajy6wubHQ1O5ZfVKj0hTNUOp1buKpa9pl51TpfCggP02d4SbTp8Qser6tQ7MUKOBpdyS6u//4d4WHCAVX2SIxQT2jitR5G9VrbAAB0sqZK9pl4X9OmkCYOTZZFFVY4GxYUHq1dihKJDgzw+3xnlphmUGwCArzte6ZAhqbK2QYlRNn2Vb9fxqjqlxYZp9YHjqq136liFQ8er6hQVEqiymnpZJB2vrNOJ6jrtLqpQW/72H9w5Wv+5Z4xHP/NMfn8zmQgAAD4m/uvxfL4Z12d4epz7vQGpZ/Yf929OmcWFB6u8pl41dU4dPl6twACLSqvq1OA0ZK+tV73TJUe9S2U19TpUUqWQ4ACVVdepvsFQhaNBDU6X6pwuHatwKMLkucooNwAAdGABVou6xIZJksKCG2tBj04RZ/WZZp8UYnQiAADgUWZfhEy5AQAAfoVyAwAA/ArlBgAA+BXKDQAA8CuUGwAA4FcoNwAAwK9QbgAAgF+h3AAAAL9CuQEAAH6FcgMAAPwK5QYAAPgVyg0AAPArlBsAAOBXAs0O0N6+mYbdbrebnAQAALTUN7+3v/k93pwOV24qKiokSWlpaSYnAQAAZ6qiokLR0dHNrmMxWlKB/IjL5VJ+fr4iIyNlsVg8+tl2u11paWnKy8tTVFSURz8b32I/tw/2c/thX7cP9nP7aKv9bBiGKioqlJqaKqu1+atqOtyRG6vVqi5durTpz4iKiuIvTjtgP7cP9nP7YV+3D/Zz+2iL/fx9R2y+wQXFAADAr1BuAACAX6HceJDNZtPcuXNls9nMjuLX2M/tg/3cftjX7YP93D68YT93uAuKAQCAf+PIDQAA8CuUGwAA4FcoNwAAwK9QbgAAgF+h3HjI/PnzlZ6erpCQEI0aNUrr1q0zO5JPmTdvnkaMGKHIyEglJibqyiuv1O7du5usU1tbq+nTpys+Pl4RERG65pprVFRU1GSd3NxcTZw4UWFhYUpMTNQvf/lLNTQ0tOdX8SmPPfaYLBaL7r33Xvcy9rNnHD16VP/zP/+j+Ph4hYaGavDgwdqwYYP7fcMwNGfOHKWkpCg0NFTZ2dnau3dvk88oLS3VlClTFBUVpZiYGN1yyy2qrKxs76/i1ZxOpx588EF1795doaGh6tmzpx5++OEm8w+xr8/cqlWrNGnSJKWmpspisWjx4sVN3vfUPt26davOP/98hYSEKC0tTX/84x898wUMnLWFCxcawcHBxksvvWTs2LHDuO2224yYmBijqKjI7Gg+Y/z48cbLL79sbN++3cjJyTEuv/xyo2vXrkZlZaV7nTvuuMNIS0szVq5caWzYsME499xzjfPOO8/9fkNDgzFo0CAjOzvb2Lx5s7FkyRIjISHBmDVrlhlfyeutW7fOSE9PN4YMGWLMmDHDvZz9fPZKS0uNbt26GTfddJOxdu1a48CBA8ayZcuMffv2udd57LHHjOjoaGPx4sXGli1bjCuuuMLo3r27UVNT417nsssuMzIyMow1a9YYn332mdGrVy/juuuuM+Mrea1HHnnEiI+PN95//33j4MGDxqJFi4yIiAjjqaeecq/Dvj5zS5YsMWbPnm288847hiTj3XffbfK+J/ZpeXm5kZSUZEyZMsXYvn27sWDBAiM0NNT429/+dtb5KTceMHLkSGP69Onu106n00hNTTXmzZtnYirfVlxcbEgyPv30U8MwDKOsrMwICgoyFi1a5F5n586dhiRj9erVhmE0/mW0Wq1GYWGhe51nn33WiIqKMhwOR/t+AS9XUVFh9O7d21i+fLlx4YUXussN+9kzfv3rXxtjxow57fsul8tITk42/vSnP7mXlZWVGTabzViwYIFhGIbx1VdfGZKM9evXu9f58MMPDYvFYhw9erTtwvuYiRMnGjfffHOTZVdffbUxZcoUwzDY157w/8uNp/bpM888Y8TGxjb5d+PXv/610bdv37POzGmps1RXV6eNGzcqOzvbvcxqtSo7O1urV682MZlvKy8vlyTFxcVJkjZu3Kj6+vom+7lfv37q2rWrez+vXr1agwcPVlJSknud8ePHy263a8eOHe2Y3vtNnz5dEydObLI/Jfazp7z33nsaPny4fvSjHykxMVGZmZl64YUX3O8fPHhQhYWFTfZzdHS0Ro0a1WQ/x8TEaPjw4e51srOzZbVatXbt2vb7Ml7uvPPO08qVK7Vnzx5J0pYtW/T5559rwoQJktjXbcFT+3T16tW64IILFBwc7F5n/Pjx2r17t06cOHFWGTvcxJmeVlJSIqfT2eQfeklKSkrSrl27TErl21wul+69916NHj1agwYNkiQVFhYqODhYMTExTdZNSkpSYWGhe51T/Tl88x4aLVy4UJs2bdL69etPeo/97BkHDhzQs88+q5kzZ+o3v/mN1q9fr5/97GcKDg7W1KlT3fvpVPvxu/s5MTGxyfuBgYGKi4tjP3/H/fffL7vdrn79+ikgIEBOp1OPPPKIpkyZIkns6zbgqX1aWFio7t27n/QZ37wXGxvb6oyUG3id6dOna/v27fr888/NjuJ38vLyNGPGDC1fvlwhISFmx/FbLpdLw4cP16OPPipJyszM1Pbt2/Xcc89p6tSpJqfzL2+99ZZef/11vfHGGxo4cKBycnJ07733KjU1lX3dgXFa6iwlJCQoICDgpLtJioqKlJycbFIq33X33Xfr/fff18cff6wuXbq4lycnJ6uurk5lZWVN1v/ufk5OTj7ln8M376HxtFNxcbHOOeccBQYGKjAwUJ9++qn+8pe/KDAwUElJSexnD0hJSdGAAQOaLOvfv79yc3Mlfbufmvt3Izk5WcXFxU3eb2hoUGlpKfv5O375y1/q/vvv109+8hMNHjxYN9xwg+677z7NmzdPEvu6LXhqn7blvyWUm7MUHBysYcOGaeXKle5lLpdLK1euVFZWlonJfIthGLr77rv17rvv6qOPPjrpUOWwYcMUFBTUZD/v3r1bubm57v2clZWlbdu2NfkLtXz5ckVFRZ30i6ajGjdunLZt26acnBz3Y/jw4ZoyZYr7Ofv57I0ePfqkoQz27Nmjbt26SZK6d++u5OTkJvvZbrdr7dq1TfZzWVmZNm7c6F7no48+ksvl0qhRo9rhW/iG6upqWa1Nf5UFBATI5XJJYl+3BU/t06ysLK1atUr19fXudZYvX66+ffue1SkpSdwK7gkLFy40bDab8corrxhfffWVcfvttxsxMTFN7iZB8+68804jOjra+OSTT4yCggL3o7q62r3OHXfcYXTt2tX46KOPjA0bNhhZWVlGVlaW+/1vblG+9NJLjZycHGPp0qVGp06duEX5e3z3binDYD97wrp164zAwEDjkUceMfbu3Wu8/vrrRlhYmPHaa6+513nssceMmJgY49///rexdetW44c//OEpb6XNzMw01q5da3z++edG7969O/TtyacydepUo3Pnzu5bwd955x0jISHB+NWvfuVeh3195ioqKozNmzcbmzdvNiQZf/7zn43Nmzcbhw8fNgzDM/u0rKzMSEpKMm644QZj+/btxsKFC42wsDBuBfcmf/3rX42uXbsawcHBxsiRI401a9aYHcmnSDrl4+WXX3avU1NTY9x1111GbGysERYWZlx11VVGQUFBk885dOiQMWHCBCM0NNRISEgwfv7znxv19fXt/G18y/8vN+xnz/jPf/5jDBo0yLDZbEa/fv2M559/vsn7LpfLePDBB42kpCTDZrMZ48aNM3bv3t1knePHjxvXXXedERERYURFRRnTpk0zKioq2vNreD273W7MmDHD6Nq1qxESEmL06NHDmD17dpPbi9nXZ+7jjz8+5b/JU6dONQzDc/t0y5YtxpgxYwybzWZ07tzZeOyxxzyS32IY3xnGEQAAwMdxzQ0AAPArlBsAAOBXKDcAAMCvUG4AAIBfodwAAAC/QrkBAAB+hXIDAAD8CuUGAAD4FcoNgA7PYrFo8eLFZscA4CGUGwCmuummm2SxWE56XHbZZWZHA+CjAs0OAACXXXaZXn755SbLbDabSWkA+DqO3AAwnc1mU3JycpNHbGyspMZTRs8++6wmTJig0NBQ9ejRQ2+//XaT7bdt26aLL75YoaGhio+P1+23367Kysom67z00ksaOHCgbDabUlJSdPfddzd5v6SkRFdddZXCwsLUu3dvvffee237pQG0GcoNAK/34IMP6pprrtGWLVs0ZcoU/eQnP9HOnTslSVVVVRo/frxiY2O1fv16LVq0SCtWrGhSXp599llNnz5dt99+u7Zt26b33ntPvXr1avIzfve73+nHP/6xtm7dqssvv1xTpkxRaWlpu35PAB7ikbnFAaCVpk6dagQEBBjh4eFNHo888ohhGIYhybjjjjuabDNq1CjjzjvvNAzDMJ5//nkjNjbWqKysdL//wQcfGFar1SgsLDQMwzBSU1ON2bNnnzaDJOOBBx5wv66srDQkGR9++KHHvieA9sM1NwBMd9FFF+nZZ59tsiwuLs79PCsrq8l7WVlZysnJkSTt3LlTGRkZCg8Pd78/evRouVwu7d69WxaLRfn5+Ro3blyzGYYMGeJ+Hh4erqioKBUXF7f2KwEwEeUGgOnCw8NPOk3kKaGhoS1aLygoqMlri8Uil8vVFpEAtDGuuQHg9dasWXPS6/79+0uS+vfvry1btqiqqsr9/hdffCGr1aq+ffsqMjJS6enpWrlyZbtmBmAejtwAMJ3D4VBhYWGTZYGBgUpISJAkLVq0SMOHD9eYMWP0+uuva926dXrxxRclSVOmTNHcuXM1depU/fa3v9WxY8d0zz336IYbblBSUpIk6be//a3uuOMOJSYmasKECaqoqNAXX3yhe+65p32/KIB2QbkBYLqlS5cqJSWlybK+fftq165dkhrvZFq4cKHuuusupaSkaMGCBRowYIAkKSwsTMuWLdOMGTM0YsQIhYWF6ZprrtGf//xn92dNnTpVtbW1euKJJ/SLX/xCCQkJuvbaa9vvCwJoVxbDMAyzQwDA6VgsFr377ru68sorzY4CwEdwzQ0AAPArlBsAAOBXuOYGgFfjzDmAM8WRGwAA4FcoNwAAwK9QbgAAgF+h3AAAAL9CuQEAAH6FcgMAAPwK5QYAAPgVyg0AAPAr/wcQkBisrlJFzQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs)\n",
        "preds.round()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLxB4CIfQLvD",
        "outputId": "d813faab-18ec-4a4d-bb70-a2db8f06b31d"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]], grad_fn=<RoundBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6x2WUrMOQUV5",
        "outputId": "bd9d164b-d087-46c0-f46c-048d4a191d6c"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    }
  ]
}